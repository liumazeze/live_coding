{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手撕 RoPE\n",
    "\n",
    "本质就是将 Q、K矩阵 乘以一个旋转矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs\\RoPE.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算 $cos(mf_i)$ 和 $sin(mf_i)$\n",
    " - $m$ 是当前 token 在其 sequence 中的位置，如 \"我爱学习\" 中，\"学\"对应的 $m$ 为 2\n",
    " - $i$ 是当前隐藏维度 hidden_size 中的位置，取值范围是 [0, $\\frac{d}{2} - 1$]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_default_rope_parameters(dim=896):\n",
    "    base = 1000000.0  # Qwen2.5: rope_theta\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / (\n",
    "        base ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim)\n",
    "    )\n",
    "\n",
    "    return inv_freq\n",
    "\n",
    "\n",
    "def rotary_emb(hidden_size, position_ids):\n",
    "    inv_freq = compute_default_rope_parameters(dim=hidden_size)\n",
    "    inv_freq_expanded = (\n",
    "        inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "    )\n",
    "    position_ids_expanded = position_ids[:, None, :].float()\n",
    "\n",
    "    freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "    emb = torch.cat((freqs, freqs), dim=-1)\n",
    "    cos = emb.cos()\n",
    "    sin = emb.sin()\n",
    "\n",
    "    return cos, sin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对 Q/K 乘以由 $cos(mf_i)$ 和 $sin(mf_i)$ 构成的旋转矩阵 (即进行旋转位置编码)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "SEQ_LEN = 4\n",
    "HIDDEN_SIZE = 32\n",
    "NUM_HEADS = 4\n",
    "\n",
    "\n",
    "position_ids = torch.tensor(\n",
    "    [\n",
    "        [0, 1, 2, 3],\n",
    "        [0, 1, 2, 3],\n",
    "    ]\n",
    ")\n",
    "cos, sin = rotary_emb(hidden_size=32, position_ids=position_ids)\n",
    "print(f\"cos/sin shape: {cos.shape}\")\n",
    "\n",
    "query_states = torch.randn(BATCH_SIZE, NUM_HEADS, SEQ_LEN, HIDDEN_SIZE)\n",
    "key_states = torch.randn(BATCH_SIZE, NUM_HEADS, SEQ_LEN, HIDDEN_SIZE)\n",
    "\n",
    "print(f\"query_states shape is: {query_states.shape}\")\n",
    "query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "print(f\"query_states(after rotary_pos_emb) shape is: {query_states.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
